# Multimodal_RAG


In this project I build a multimodal Retrieval-Augmented Generation (RAG) pipeline using LangChain and the Unstructured library. Here I create an AI-powered system that can query complex documents, such as PDFs containing text, images, tables, and plots, by harnessing the multimodal capabilities of advanced Language Learning Models (LLMs) like GPT-4 with vision.

I started with setting up the Unstructured library to parse and pre-process various document formats, from images to text. Then, I use LangChain to establish a document retrieval system that integrates textual and visual data into a multimodal LLM, enabling comprehensive understanding and accurate, relevant responses. This method is perfect for tasks requiring insights across multiple data formats, such as technical documents, scientific papers, and presentations.
